using cross
Using script examples/pretrain_gpt_sw_allreduce.sh
Using random 1 nodes
Job <3384322> has been submitted to queue <q_formal_cesm>
waiting for dispatch ...
[3384322 ## mn290: 7209 ## 2022-01-30/05:28:40] after query job cnodelist from db, use-time 0.26 secs.
[3384322 ## mn290: 7209 ## 2022-01-30/05:28:40] cnodenum= 64, job-resource-list: 25728-25791[mpemap:0x1][spemap:0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF]
[3384322 ## mn290: 7209 ## 2022-01-30/05:28:40] control node is: mn290.
[3384322 ## mn290: 7209 ## 2022-01-30/05:28:40] sub control node number is 1, list: mn218.
[3384322 ## mn290: 7209 ## 2022-01-30/05:28:40] after gather all tasks. use-time: 0.20 secs.
[3384322 ## mn290: 7209 ## 2022-01-30/05:28:40] after put tasks info to db, use-time 0.05 secs.
[3384322 ## mn290: 7209 ## 2022-01-30/05:28:40] start job ok, cnodenum=64, total use-time=0.64 secs!
vn025728: no optimizer found. set to paro
vn025728: using world size: 64, data-parallel-size: 64, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
vn025728: using torch.float32 for parameters ...
vn025728: WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:BertWordPieceLowerCase
vn025728: ------------------------ arguments ------------------------
vn025728:   adam_beta1 ...................................... 0.90000000000000002
vn025728:   adam_beta2 ...................................... 0.999
vn025728:   adam_eps ........................................ 1e-08
vn025728:   adlr_autoresume ................................. False
vn025728:   adlr_autoresume_interval ........................ 1000
vn025728:   apply_query_key_layer_scaling ................... True
vn025728:   apply_residual_connection_post_layernorm ........ False
vn025728:   attention_dropout ............................... 0.10000000000000001
vn025728:   attention_softmax_in_fp32 ....................... False
vn025728:   balance_loss_weight ............................. 1
vn025728:   balance_strategy ................................ swipe
vn025728:   bert_load ....................................... None
vn025728:   bf16 ............................................ False
vn025728:   bias_dropout_fusion ............................. False
vn025728:   bias_gelu_fusion ................................ False
vn025728:   block_data_path ................................. None
vn025728:   checkpoint_activations .......................... False
vn025728:   checkpoint_num_layers ........................... 1
vn025728:   clip_grad ....................................... 1.0
vn025728:   concat_two ...................................... False
vn025728:   consumed_train_samples .......................... 0
vn025728:   consumed_valid_samples .......................... 0
vn025728:   crazy ........................................... False
vn025728:   crazy_dp ........................................ -1
vn025728:   data_impl ....................................... mmap
vn025728:   data_parallel_size .............................. 64
vn025728:   data_path ....................................... None
vn025728:   DDP_impl ........................................ local
vn025728:   distribute_checkpointed_activations ............. False
vn025728:   distributed_backend ............................. mpi
vn025728:   eod_mask_loss ................................... True
vn025728:   eval_interval ................................... 1000
vn025728:   eval_iters ...................................... 10
vn025728:   exit_duration_in_mins ........................... None
vn025728:   exit_interval ................................... None
vn025728:   faiss_use_gpu ................................... False
vn025728:   finetune ........................................ False
vn025728:   fmoefy .......................................... False
vn025728:   fp16 ............................................ False
vn025728:   fp32_allreduce .................................. False
vn025728:   fp32_dynamic_loss_scale ......................... False
vn025728:   fp32_layernorm .................................. False
vn025728:   fp32_residual_connection ........................ False
vn025728:   fp32_static_loss_scale .......................... 1.0
vn025728:   generate_interval ............................... 2000
vn025728:   generate_prompt_length .......................... 32
vn025728:   generate_steps .................................. 128
vn025728:   global_batch_size ............................... 64
vn025728:   half_lm_cross_entropy ........................... False
vn025728:   hidden_dropout .................................. 0.10000000000000001
vn025728:   hidden_hidden_size .............................. 4096
vn025728:   hidden_size ..................................... 768
vn025728:   hysteresis ...................................... 2
vn025728:   ict_head_size ................................... None
vn025728:   ict_load ........................................ None
vn025728:   indexer_batch_size .............................. 128
vn025728:   indexer_log_interval ............................ 1000
vn025728:   init_method_std ................................. 0.01
vn025728:   initial_loss_scale .............................. 4294967296
vn025728:   layernorm_epsilon ............................... 1.0000000000000001e-05
vn025728:   lazy_mpu_init ................................... None
vn025728:   load ............................................ ./ckpt/m6-220130-052838
vn025728:   local_rank ...................................... None
vn025728:   log_interval .................................... 1
vn025728:   loss_scale ...................................... None
vn025728:   loss_scale_window ............................... 1000
vn025728:   lr .............................................. 2.0000000000000002e-05
vn025728:   lr_decay_iters .................................. 100000
vn025728:   lr_decay_samples ................................ None
vn025728:   lr_decay_style .................................. linear
vn025728:   lr_warmup_fraction .............................. 0.029999999999999999
vn025728:   lr_warmup_iters ................................. 0
vn025728:   lr_warmup_samples ............................... 0
vn025728:   m6_img_feature_dim .............................. 2048
vn025728:   m6_img_file_list ................................ None
vn025728:   m6_patch_nums ................................... 16
vn025728:   m6_txt_file_list ................................ ./dataset/all_imgtxttoken_files.txt
vn025728:   make_vocab_size_divisible_by .................... 512
vn025728:   mask_prob ....................................... 0.14999999999999999
vn025728:   max_position_embeddings ......................... 1152
vn025728:   merge_file ...................................... None
vn025728:   micro_batch_size ................................ 1
vn025728:   min_loss_scale .................................. 1.0
vn025728:   min_lr .......................................... 4.9999999999999998e-07
vn025728:   mmap_warmup ..................................... False
vn025728:   moe_size ........................................ 256
vn025728:   no_load_optim ................................... False
vn025728:   no_load_rng ..................................... False
vn025728:   no_mid_align .................................... False
vn025728:   no_save_optim ................................... False
vn025728:   no_save_rng ..................................... False
vn025728:   num_attention_heads ............................. 16
vn025728:   num_experts ..................................... 1
vn025728:   num_layers ...................................... 6
vn025728:   num_workers ..................................... 0
vn025728:   onnx_safe ....................................... None
vn025728:   openai_gelu ..................................... False
vn025728:   optimizer_type .................................. none
vn025728:   override_lr_scheduler ........................... True
vn025728:   pad_mask_loss ................................... True
vn025728:   params_dtype .................................... torch.float32
vn025728:   pipeline_model_parallel_size .................... 1
vn025728:   print_ops ....................................... False
vn025728:   print_ops_freq .................................. -1
vn025728:   prompt_length ................................... 512
vn025728:   query_in_block_prob ............................. 0.10000000000000001
vn025728:   rampup_batch_size ............................... None
vn025728:   rank ............................................ 0
vn025728:   report_topk_accuracies .......................... []
vn025728:   reset_attention_mask ............................ False
vn025728:   reset_position_ids .............................. False
vn025728:   save ............................................ ./ckpt/m6-220130-052838
vn025728:   save_interval ................................... 200
vn025728:   scaled_masked_softmax_fusion .................... False
vn025728:   scaled_upper_triang_masked_softmax_fusion ....... False
vn025728:   seed ............................................ 1234
vn025728:   seq_length ...................................... 1152
vn025728:   share_gate ...................................... False
vn025728:   short_seq_prob .................................. 0.10000000000000001
vn025728:   sn_size ......................................... 256
vn025728:   split ........................................... 990,9,1
vn025728:   sw_align ........................................ 192
vn025728:   temperature ..................................... 0.90000000000000002
vn025728:   tensor_model_parallel_size ...................... 1
vn025728:   tensorboard_dir ................................. None
vn025728:   titles_data_path ................................ None
vn025728:   tokenizer_type .................................. BertWordPieceLowerCase
vn025728:   top_k ........................................... 2
vn025728:   train_iters ..................................... 500000
vn025728:   train_samples ................................... None
vn025728:   use_checkpoint_lr_scheduler ..................... False
vn025728:   use_cpu_initialization .......................... True
vn025728:   use_one_sent_docs ............................... False
vn025728:   vocab_file ...................................... ./dataset/merged_vocab.txt
vn025728:   weight_decay .................................... 0.01
vn025728:   world_size ...................................... 64
vn025728:   zero_gate ....................................... False
vn025728: -------------------- end of arguments ---------------------
vn025728: setting number of micro-batches to constant 1
vn025728: > building BertWordPieceLowerCase tokenizer ...
vn025728:  > padded vocab (size: 209221) with 187 dummy tokens (new size: 209408)
vn025728: > initializing torch distributed ...
vn025728: > initializing tensor model parallel with size 1
vn025728: > initializing pipeline model parallel with size 1
vn025728: > Megatron distributed init time 0.030236721038818359 s
vn025728: > setting random seeds to 1234 ...
vn025728: time to initialize megatron (seconds): 77.392
vn025728: [after megatron is initialized] datetime: 2022-01-30 05:29:49 
vn025728: building GPT model ...
vn025728:  > number of parameters with row size 256: 2.042388e+08
vn025791: time (ms) | build model: 787.53
vn025728: > learning rate decay style: linear
vn025728: > building train, validation, and test datasets ...
vn025728:  > datasets target sizes (minimum size):
vn025728:     train:      32000000
vn025728:     validation: 320640
vn025728:     test:       640
vn025728: > building train, validation, and test datasets for GPT ...
vn025728:     train:
vn025728:       file list indices in [0, 64) total of 64 files
vn025728:     valid:
vn025728:       file list indices in [64, 65) total of 1 files
vn025728:     test:
vn025728:       file list indices in [65, 65) total of 0 files
vn025791: Loading checkpoint from ./ckpt/m6-220130-052838
vn025791: WARNING: could not find the metadata file ./ckpt/m6-220130-052838/latest_checkpointed_iteration.txt 
vn025791:     will not load any checkpoints and will start from random
vn025791: time (ms) | load checkpoint: 2.91
vn025728: > finished creating GPT (SWLoader) datasets ...
vn025791: time (ms) | model and optimizer: 807.25 | train/valid/test data iterators: 785.26
vn025728: [after dataloaders are built] datetime: 2022-01-30 05:29:50 
vn025728: [after model, optimizer, and learning rate scheduler are built] datetime: 2022-01-30 05:29:50 
vn025728: done with setups ...
vn025728: training ...
vn025728: [before the start of training step] datetime: 2022-01-30 05:29:50 
vn025728: forwarding 0. Mem: 0.761 GB
vn025728: backwarding 0. Mem: 3.925 GB
vn025728: optimizing. Mem: 1.5212526172399521 GB
vn025791:  iteration        1/  500000 | consumed samples:           64 | elapsed time per iteration (ms): 2604.6 | total FLOPS: 0.000000e+00 | learning rate: 6.667E-09 | global batch size:    64 | lm loss: 1.230507E+01 | txt loss: 1.230508E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 3.925 GB |
vn025791: time (ms) | forward-compute: 443.71 | backward-compute: 397.51 | backward-params-all-reduce: 1489.78 | backward-embedding-all-reduce: 0.09 | optimizer: 264.81 | optimizer-compute: 191.59 | batch-generator: 41.59
vn025728: memory report func mocked
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration        2/  500000 | consumed samples:          128 | elapsed time per iteration (ms): 2550.7 | total FLOPS: 4.119069e+13 | learning rate: 1.333E-08 | global batch size:    64 | lm loss: 1.231212E+01 | txt loss: 1.231212E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 441.19 | backward-compute: 384.80 | backward-params-all-reduce: 1472.86 | backward-embedding-all-reduce: 0.09 | optimizer: 241.09 | optimizer-compute: 168.22 | batch-generator: 41.34
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration        3/  500000 | consumed samples:          192 | elapsed time per iteration (ms): 2524.2 | total FLOPS: 4.162303e+13 | learning rate: 2.000E-08 | global batch size:    64 | lm loss: 1.231087E+01 | txt loss: 1.231087E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 439.03 | backward-compute: 383.84 | backward-params-all-reduce: 1449.95 | backward-embedding-all-reduce: 0.09 | optimizer: 241.07 | optimizer-compute: 168.22 | batch-generator: 41.13
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration        4/  500000 | consumed samples:          256 | elapsed time per iteration (ms): 2523.1 | total FLOPS: 4.164111e+13 | learning rate: 2.667E-08 | global batch size:    64 | lm loss: 1.231399E+01 | txt loss: 1.231398E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 438.91 | backward-compute: 384.00 | backward-params-all-reduce: 1447.92 | backward-embedding-all-reduce: 0.15 | optimizer: 241.65 | optimizer-compute: 168.47 | batch-generator: 41.36
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration        5/  500000 | consumed samples:          320 | elapsed time per iteration (ms): 2513.8 | total FLOPS: 4.179575e+13 | learning rate: 3.333E-08 | global batch size:    64 | lm loss: 1.230707E+01 | txt loss: 1.230706E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 438.80 | backward-compute: 383.66 | backward-params-all-reduce: 1439.87 | backward-embedding-all-reduce: 0.09 | optimizer: 241.18 | optimizer-compute: 168.27 | batch-generator: 40.65
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration        6/  500000 | consumed samples:          384 | elapsed time per iteration (ms): 2539.6 | total FLOPS: 4.137136e+13 | learning rate: 4.000E-08 | global batch size:    64 | lm loss: 1.230575E+01 | txt loss: 1.230574E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 440.56 | backward-compute: 383.95 | backward-params-all-reduce: 1462.64 | backward-embedding-all-reduce: 0.09 | optimizer: 241.66 | optimizer-compute: 168.61 | batch-generator: 43.14
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration        7/  500000 | consumed samples:          448 | elapsed time per iteration (ms): 2519.6 | total FLOPS: 4.169943e+13 | learning rate: 4.667E-08 | global batch size:    64 | lm loss: 1.230698E+01 | txt loss: 1.230698E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 437.82 | backward-compute: 384.07 | backward-params-all-reduce: 1445.61 | backward-embedding-all-reduce: 0.09 | optimizer: 241.87 | optimizer-compute: 168.58 | batch-generator: 40.72
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration        8/  500000 | consumed samples:          512 | elapsed time per iteration (ms): 2515.5 | total FLOPS: 4.176763e+13 | learning rate: 5.333E-08 | global batch size:    64 | lm loss: 1.230452E+01 | txt loss: 1.230451E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 437.68 | backward-compute: 384.37 | backward-params-all-reduce: 1441.27 | backward-embedding-all-reduce: 0.09 | optimizer: 241.36 | optimizer-compute: 168.39 | batch-generator: 40.85
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration        9/  500000 | consumed samples:          576 | elapsed time per iteration (ms): 2524.2 | total FLOPS: 4.162405e+13 | learning rate: 6.000E-08 | global batch size:    64 | lm loss: 1.230632E+01 | txt loss: 1.230632E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 432.29 | backward-compute: 384.42 | backward-params-all-reduce: 1456.49 | backward-embedding-all-reduce: 0.09 | optimizer: 240.81 | optimizer-compute: 168.07 | batch-generator: 40.83
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       10/  500000 | consumed samples:          640 | elapsed time per iteration (ms): 2515.4 | total FLOPS: 4.176930e+13 | learning rate: 6.667E-08 | global batch size:    64 | lm loss: 1.229929E+01 | txt loss: 1.229928E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 437.36 | backward-compute: 383.88 | backward-params-all-reduce: 1441.50 | backward-embedding-all-reduce: 0.09 | optimizer: 241.86 | optimizer-compute: 168.68 | batch-generator: 41.42
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       11/  500000 | consumed samples:          704 | elapsed time per iteration (ms): 2508.1 | total FLOPS: 4.189051e+13 | learning rate: 7.333E-08 | global batch size:    64 | lm loss: 1.229685E+01 | txt loss: 1.229685E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 436.73 | backward-compute: 384.55 | backward-params-all-reduce: 1434.91 | backward-embedding-all-reduce: 0.09 | optimizer: 241.57 | optimizer-compute: 168.27 | batch-generator: 40.80
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       12/  500000 | consumed samples:          768 | elapsed time per iteration (ms): 2508.5 | total FLOPS: 4.188425e+13 | learning rate: 8.000E-08 | global batch size:    64 | lm loss: 1.229698E+01 | txt loss: 1.229698E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 436.97 | backward-compute: 385.11 | backward-params-all-reduce: 1434.51 | backward-embedding-all-reduce: 0.09 | optimizer: 241.24 | optimizer-compute: 168.39 | batch-generator: 41.03
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       13/  500000 | consumed samples:          832 | elapsed time per iteration (ms): 2512.7 | total FLOPS: 4.181463e+13 | learning rate: 8.667E-08 | global batch size:    64 | lm loss: 1.229544E+01 | txt loss: 1.229544E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 428.65 | backward-compute: 384.49 | backward-params-all-reduce: 1447.50 | backward-embedding-all-reduce: 0.09 | optimizer: 241.60 | optimizer-compute: 168.42 | batch-generator: 40.70
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       14/  500000 | consumed samples:          896 | elapsed time per iteration (ms): 2536.2 | total FLOPS: 4.142708e+13 | learning rate: 9.333E-08 | global batch size:    64 | lm loss: 1.229429E+01 | txt loss: 1.229429E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 438.90 | backward-compute: 384.67 | backward-params-all-reduce: 1460.43 | backward-embedding-all-reduce: 0.09 | optimizer: 241.52 | optimizer-compute: 168.35 | batch-generator: 41.76
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       15/  500000 | consumed samples:          960 | elapsed time per iteration (ms): 2504.3 | total FLOPS: 4.195414e+13 | learning rate: 1.000E-07 | global batch size:    64 | lm loss: 1.228615E+01 | txt loss: 1.228614E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 437.27 | backward-compute: 384.33 | backward-params-all-reduce: 1430.22 | backward-embedding-all-reduce: 0.09 | optimizer: 242.13 | optimizer-compute: 168.85 | batch-generator: 41.41
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       16/  500000 | consumed samples:         1024 | elapsed time per iteration (ms): 2510.7 | total FLOPS: 4.184742e+13 | learning rate: 1.067E-07 | global batch size:    64 | lm loss: 1.228905E+01 | txt loss: 1.228905E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 437.20 | backward-compute: 384.28 | backward-params-all-reduce: 1436.33 | backward-embedding-all-reduce: 0.09 | optimizer: 242.05 | optimizer-compute: 168.79 | batch-generator: 40.83
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       17/  500000 | consumed samples:         1088 | elapsed time per iteration (ms): 2511.4 | total FLOPS: 4.183643e+13 | learning rate: 1.133E-07 | global batch size:    64 | lm loss: 1.227961E+01 | txt loss: 1.227961E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 436.92 | backward-compute: 384.66 | backward-params-all-reduce: 1437.54 | backward-embedding-all-reduce: 0.09 | optimizer: 241.95 | optimizer-compute: 168.93 | batch-generator: 41.18
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       18/  500000 | consumed samples:         1152 | elapsed time per iteration (ms): 2521.9 | total FLOPS: 4.166104e+13 | learning rate: 1.200E-07 | global batch size:    64 | lm loss: 1.227911E+01 | txt loss: 1.227910E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 445.26 | backward-compute: 383.89 | backward-params-all-reduce: 1439.80 | backward-embedding-all-reduce: 0.09 | optimizer: 242.18 | optimizer-compute: 168.90 | batch-generator: 48.30
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       19/  500000 | consumed samples:         1216 | elapsed time per iteration (ms): 2529.8 | total FLOPS: 4.153211e+13 | learning rate: 1.267E-07 | global batch size:    64 | lm loss: 1.227278E+01 | txt loss: 1.227278E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 437.43 | backward-compute: 385.04 | backward-params-all-reduce: 1455.68 | backward-embedding-all-reduce: 0.09 | optimizer: 241.28 | optimizer-compute: 168.36 | batch-generator: 40.69
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       20/  500000 | consumed samples:         1280 | elapsed time per iteration (ms): 2508.6 | total FLOPS: 4.188287e+13 | learning rate: 1.333E-07 | global batch size:    64 | lm loss: 1.227099E+01 | txt loss: 1.227100E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 439.92 | backward-compute: 384.29 | backward-params-all-reduce: 1431.68 | backward-embedding-all-reduce: 0.09 | optimizer: 241.85 | optimizer-compute: 168.61 | batch-generator: 43.46
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       21/  500000 | consumed samples:         1344 | elapsed time per iteration (ms): 2587.1 | total FLOPS: 4.061150e+13 | learning rate: 1.400E-07 | global batch size:    64 | lm loss: 1.227140E+01 | txt loss: 1.227140E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 438.11 | backward-compute: 384.14 | backward-params-all-reduce: 1512.72 | backward-embedding-all-reduce: 0.09 | optimizer: 241.78 | optimizer-compute: 168.65 | batch-generator: 50.72
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       22/  500000 | consumed samples:         1408 | elapsed time per iteration (ms): 2590.1 | total FLOPS: 4.056531e+13 | learning rate: 1.467E-07 | global batch size:    64 | lm loss: 1.225757E+01 | txt loss: 1.225756E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 438.33 | backward-compute: 384.57 | backward-params-all-reduce: 1514.79 | backward-embedding-all-reduce: 0.09 | optimizer: 241.46 | optimizer-compute: 168.50 | batch-generator: 41.37
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       23/  500000 | consumed samples:         1472 | elapsed time per iteration (ms): 2516.5 | total FLOPS: 4.175059e+13 | learning rate: 1.533E-07 | global batch size:    64 | lm loss: 1.225496E+01 | txt loss: 1.225496E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 438.27 | backward-compute: 384.21 | backward-params-all-reduce: 1442.08 | backward-embedding-all-reduce: 0.09 | optimizer: 241.68 | optimizer-compute: 168.70 | batch-generator: 41.77
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       24/  500000 | consumed samples:         1536 | elapsed time per iteration (ms): 2521.9 | total FLOPS: 4.166220e+13 | learning rate: 1.600E-07 | global batch size:    64 | lm loss: 1.225088E+01 | txt loss: 1.225088E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 438.55 | backward-compute: 384.07 | backward-params-all-reduce: 1446.79 | backward-embedding-all-reduce: 0.09 | optimizer: 241.68 | optimizer-compute: 168.68 | batch-generator: 41.54
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       25/  500000 | consumed samples:         1600 | elapsed time per iteration (ms): 2507.5 | total FLOPS: 4.190093e+13 | learning rate: 1.667E-07 | global batch size:    64 | lm loss: 1.225063E+01 | txt loss: 1.225062E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 433.10 | backward-compute: 384.17 | backward-params-all-reduce: 1438.33 | backward-embedding-all-reduce: 0.09 | optimizer: 241.56 | optimizer-compute: 168.80 | batch-generator: 40.93
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       26/  500000 | consumed samples:         1664 | elapsed time per iteration (ms): 2512.9 | total FLOPS: 4.181031e+13 | learning rate: 1.733E-07 | global batch size:    64 | lm loss: 1.224218E+01 | txt loss: 1.224218E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 439.66 | backward-compute: 384.78 | backward-params-all-reduce: 1435.90 | backward-embedding-all-reduce: 0.09 | optimizer: 241.75 | optimizer-compute: 168.81 | batch-generator: 42.31
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       27/  500000 | consumed samples:         1728 | elapsed time per iteration (ms): 2536.2 | total FLOPS: 4.142726e+13 | learning rate: 1.800E-07 | global batch size:    64 | lm loss: 1.223405E+01 | txt loss: 1.223405E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 438.27 | backward-compute: 384.74 | backward-params-all-reduce: 1461.24 | backward-embedding-all-reduce: 0.09 | optimizer: 241.60 | optimizer-compute: 168.70 | batch-generator: 41.29
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       28/  500000 | consumed samples:         1792 | elapsed time per iteration (ms): 2515.8 | total FLOPS: 4.176316e+13 | learning rate: 1.867E-07 | global batch size:    64 | lm loss: 1.222958E+01 | txt loss: 1.222958E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 439.83 | backward-compute: 384.19 | backward-params-all-reduce: 1438.56 | backward-embedding-all-reduce: 0.09 | optimizer: 242.19 | optimizer-compute: 168.98 | batch-generator: 43.43
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       29/  500000 | consumed samples:         1856 | elapsed time per iteration (ms): 2532.3 | total FLOPS: 4.149140e+13 | learning rate: 1.933E-07 | global batch size:    64 | lm loss: 1.222947E+01 | txt loss: 1.222947E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 437.57 | backward-compute: 383.99 | backward-params-all-reduce: 1458.17 | backward-embedding-all-reduce: 0.09 | optimizer: 242.19 | optimizer-compute: 169.06 | batch-generator: 40.74
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       30/  500000 | consumed samples:         1920 | elapsed time per iteration (ms): 2514.2 | total FLOPS: 4.178994e+13 | learning rate: 2.000E-07 | global batch size:    64 | lm loss: 1.221615E+01 | txt loss: 1.221615E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 437.04 | backward-compute: 384.11 | backward-params-all-reduce: 1440.22 | backward-embedding-all-reduce: 0.09 | optimizer: 242.06 | optimizer-compute: 168.82 | batch-generator: 40.90
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
vn025791:  iteration       31/  500000 | consumed samples:         1984 | elapsed time per iteration (ms): 2515.5 | total FLOPS: 4.176783e+13 | learning rate: 2.067E-07 | global batch size:    64 | lm loss: 1.220781E+01 | txt loss: 1.220781E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 438.93 | backward-compute: 384.42 | backward-params-all-reduce: 1439.52 | backward-embedding-all-reduce: 0.09 | optimizer: 242.17 | optimizer-compute: 169.04 | batch-generator: 41.95
vn025728: forwarding 0. Mem: 2.282 GB
vn025728: backwarding 0. Mem: 5.446 GB
vn025728: optimizing. Mem: 3.0425052493810654 GB
[3384322 ## mn290: 7206 ## Sun Jan 30 05:31:11 2022]  jobstarter_sig_opt: sig-action is BKILL
[3384322 ## 2022-01-30/05:31:11] receive request of kill job(bkill).
vn025791:  iteration       32/  500000 | consumed samples:         2048 | elapsed time per iteration (ms): 2530.5 | total FLOPS: 4.151980e+13 | learning rate: 2.133E-07 | global batch size:    64 | lm loss: 1.220310E+01 | txt loss: 1.220310E+01 | loss scale: 1.0 | number of skipped iterations:   0 | number of nan iterations:   0 | allocated memory cur/max: 3.043 5.446 GB |
vn025791: time (ms) | forward-compute: 438.34 | backward-compute: 384.49 | backward-params-all-reduce: 1454.96 | backward-embedding-all-reduce: 0.09 | optimizer: 241.89 | optimizer-compute: 168.58 | batch-generator: 40.78
[3384322 ## mn290: 7209 ## 2022-01-30/05:31:25] job_finished. use-time: 164.492134 (secs)
dispatching ...
Job 3384322 has been finished, exit reason: bkill.
