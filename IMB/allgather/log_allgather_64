************************************************ export SWCH_USE_SLAVE_COMMU=0;  export SWCH_USE_META=0 **************************************************
Job <3491613> has been submitted to queue <q_share>
waiting for dispatch ...
[3491613 ## mn428: 16672 ## 2022-04-08/00:00:24] after query job cnodelist from db, use-time 0.17 secs.
[3491613 ## mn428: 16672 ## 2022-04-08/00:00:24] cnodenum= 16, job-resource-list: 5120-5135[mpemap:0xf][spemap:0x00000000000000000000000000000000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF]
[3491613 ## mn428: 16672 ## 2022-04-08/00:00:24] control node is: mn428.
[3491613 ## mn428: 16672 ## 2022-04-08/00:00:24] sub control node number is 1, list: mn187.
[3491613 ## mn428: 16672 ## 2022-04-08/00:00:24] after gather all tasks. use-time: 0.51 secs.
[3491613 ## mn428: 16672 ## 2022-04-08/00:00:26] after put tasks info to db, use-time 1.33 secs.
[3491613 ## mn428: 16672 ## 2022-04-08/00:00:26] start job ok, cnodenum=16, total use-time=2.17 secs!
[3491613 ## vn005120: 25248 ## 2022-04-08/00:00:28] job's first task will run: /usr/bin/swrun -b 1 -T 1024 -s 8000 -c 0 -j 3491613 -F 0xFFFFFFFFFFFFFFFF -i ./IMB-MPI1 -iter 10000,5120,20 -npmin 16384 allgather -msglen len_allgather -sync 1 -mem 4.0 
 benchmarks to run allgather 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.1, MPI-1 part    
#------------------------------------------------------------
# Date                  : Fri Apr  8 00:00:30 2022
# Machine               : sw_64
# System                : Linux
# Release               : 4.4.15-g6fff048
# Version               : #239 SMP Fri May 14 16:06:39 CST 2021
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# ./IMB-MPI1 -iter 10000 -npmin 16384 allgather -msglen len_allgather
#            -sync 1 -mem 4.0

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Allgather

#----------------------------------------------------------------
# Benchmarking Allgather 
# #processes = 64 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
         2048       100093        93.20        97.82        96.08	Fri Apr  8 00:00:43 2022
         8192        64788       149.76       155.36       152.96	Fri Apr  8 00:00:56 2022
        32768        25465       388.40       395.63       392.44	Fri Apr  8 00:01:07 2022
       131072         7301      1361.47      1374.14      1367.20	Fri Apr  8 00:01:19 2022
       524288         1797      5426.44      5600.18      5526.74	Fri Apr  8 00:01:30 2022


# All processes entering MPI_Finalize

[3491613 ## mn428: 16672 ## 2022-04-08/00:01:53] job_finished. use-time: 87.374300 (secs)
dispatching ...
Job 3491613 has been finished.
************************************************ export SWCH_USE_SLAVE_COMMU=1;  export SWCH_USE_META=0 **************************************************
Job <3491616> has been submitted to queue <q_share>
waiting for dispatch ...
[3491616 ## mn431: 21872 ## 2022-04-08/00:02:17] after query job cnodelist from db, use-time 0.17 secs.
[3491616 ## mn431: 21872 ## 2022-04-08/00:02:17] cnodenum= 16, job-resource-list: 5120-5135[mpemap:0xf][spemap:0x00000000000000000000000000000000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF]
[3491616 ## mn431: 21872 ## 2022-04-08/00:02:17] control node is: mn431.
[3491616 ## mn431: 21872 ## 2022-04-08/00:02:17] sub control node number is 1, list: mn184.
[3491616 ## mn431: 21872 ## 2022-04-08/00:02:17] after gather all tasks. use-time: 0.51 secs.
[3491616 ## mn431: 21872 ## 2022-04-08/00:02:17] after put tasks info to db, use-time 0.06 secs.
[3491616 ## mn431: 21872 ## 2022-04-08/00:02:17] start job ok, cnodenum=16, total use-time=0.83 secs!
[3491616 ## vn005120: 25295 ## 2022-04-08/00:02:19] job's first task will run: /usr/bin/swrun -b 1 -T 1024 -s 8000 -c 0 -j 3491616 -F 0xFFFFFFFFFFFFFFFF -i ./IMB-MPI1 -iter 10000,5120,20 -npmin 16384 allgather -msglen len_allgather -sync 1 -mem 4.0 
 benchmarks to run allgather 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.1, MPI-1 part    
#------------------------------------------------------------
# Date                  : Fri Apr  8 00:02:21 2022
# Machine               : sw_64
# System                : Linux
# Release               : 4.4.15-g6fff048
# Version               : #239 SMP Fri May 14 16:06:39 CST 2021
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# ./IMB-MPI1 -iter 10000 -npmin 16384 allgather -msglen len_allgather
#            -sync 1 -mem 4.0

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Allgather

#----------------------------------------------------------------
# Benchmarking Allgather 
# #processes = 64 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
         2048        99536        93.33        97.32        95.88	Fri Apr  8 00:02:34 2022
         8192        64242       149.35       154.70       152.36	Fri Apr  8 00:02:47 2022
        32768        25420       388.36       394.99       392.18	Fri Apr  8 00:02:58 2022
       131072         7341      1346.37      1371.53      1360.24	Fri Apr  8 00:03:10 2022
       524288         1786      5187.40      5596.66      5480.34	Fri Apr  8 00:03:21 2022


# All processes entering MPI_Finalize

[3491616 ## mn431: 21872 ## 2022-04-08/00:03:43] job_finished. use-time: 85.394207 (secs)
dispatching ...
Job 3491616 has been finished.
************************************************ export SWCH_USE_SLAVE_COMMU=0;  export SWCH_USE_META=1 **************************************************
Job <3491619> has been submitted to queue <q_share>
waiting for dispatch ...
[3491619 ## mn002: 586 ## 2022-04-08/00:04:05] after query job cnodelist from db, use-time 0.18 secs.
[3491619 ## mn002: 586 ## 2022-04-08/00:04:05] cnodenum= 16, job-resource-list: 5120-5135[mpemap:0xf][spemap:0x00000000000000000000000000000000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF]
[3491619 ## mn002: 586 ## 2022-04-08/00:04:05] control node is: mn002.
[3491619 ## mn002: 586 ## 2022-04-08/00:04:05] sub control node number is 1, list: mn058.
[3491619 ## mn002: 586 ## 2022-04-08/00:04:06] after gather all tasks. use-time: 0.51 secs.
[3491619 ## mn002: 586 ## 2022-04-08/00:04:06] after put tasks info to db, use-time 0.06 secs.
[3491619 ## mn002: 586 ## 2022-04-08/00:04:06] start job ok, cnodenum=16, total use-time=0.81 secs!
[3491619 ## vn005120: 25336 ## 2022-04-08/00:04:08] job's first task will run: /usr/bin/swrun -b 1 -T 1024 -s 8000 -c 0 -j 3491619 -F 0xFFFFFFFFFFFFFFFF -i ./IMB-MPI1 -iter 10000,5120,20 -npmin 16384 allgather -msglen len_allgather -sync 1 -mem 4.0 
 benchmarks to run allgather 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.1, MPI-1 part    
#------------------------------------------------------------
# Date                  : Fri Apr  8 00:04:10 2022
# Machine               : sw_64
# System                : Linux
# Release               : 4.4.15-g6fff048
# Version               : #239 SMP Fri May 14 16:06:39 CST 2021
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# ./IMB-MPI1 -iter 10000 -npmin 16384 allgather -msglen len_allgather
#            -sync 1 -mem 4.0

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Allgather

#----------------------------------------------------------------
# Benchmarking Allgather 
# #processes = 64 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
         2048        87318       110.36       113.34       112.45	Fri Apr  8 00:04:23 2022
         8192        61060       161.50       165.30       163.74	Fri Apr  8 00:04:35 2022
        32768        25736       386.40       389.87       388.57	Fri Apr  8 00:04:47 2022
       131072         7558      1319.25      1323.56      1321.34	Fri Apr  8 00:04:58 2022
       524288         1949      5108.59      5154.35      5130.35	Fri Apr  8 00:05:09 2022


# All processes entering MPI_Finalize

[3491619 ## mn002: 586 ## 2022-04-08/00:05:24] job_finished. use-time: 78.456475 (secs)
dispatching ...
Job 3491619 has been finished.
************************************************ export SWCH_USE_SLAVE_COMMU=1;  export SWCH_USE_META=1 **************************************************
Job <3491620> has been submitted to queue <q_share>
waiting for dispatch ...
[3491620 ## mn003: 32173 ## 2022-04-08/00:05:48] after query job cnodelist from db, use-time 0.20 secs.
[3491620 ## mn003: 32173 ## 2022-04-08/00:05:48] cnodenum= 16, job-resource-list: 5120-5135[mpemap:0xf][spemap:0x00000000000000000000000000000000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF]
[3491620 ## mn003: 32173 ## 2022-04-08/00:05:48] control node is: mn003.
[3491620 ## mn003: 32173 ## 2022-04-08/00:05:48] sub control node number is 1, list: mn167.
[3491620 ## mn003: 32173 ## 2022-04-08/00:05:48] after gather all tasks. use-time: 0.50 secs.
[3491620 ## mn003: 32173 ## 2022-04-08/00:05:48] after put tasks info to db, use-time 0.08 secs.
[3491620 ## mn003: 32173 ## 2022-04-08/00:05:48] start job ok, cnodenum=16, total use-time=0.91 secs!
[3491620 ## vn005120: 25378 ## 2022-04-08/00:05:51] job's first task will run: /usr/bin/swrun -b 1 -T 1024 -s 8000 -c 0 -j 3491620 -F 0xFFFFFFFFFFFFFFFF -i ./IMB-MPI1 -iter 10000,5120,20 -npmin 16384 allgather -msglen len_allgather -sync 1 -mem 4.0 
 benchmarks to run allgather 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.1, MPI-1 part    
#------------------------------------------------------------
# Date                  : Fri Apr  8 00:05:53 2022
# Machine               : sw_64
# System                : Linux
# Release               : 4.4.15-g6fff048
# Version               : #239 SMP Fri May 14 16:06:39 CST 2021
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# ./IMB-MPI1 -iter 10000 -npmin 16384 allgather -msglen len_allgather
#            -sync 1 -mem 4.0

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Allgather

#----------------------------------------------------------------
# Benchmarking Allgather 
# #processes = 64 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
         2048        88271       110.43       113.48       112.51	Fri Apr  8 00:06:06 2022
         8192        67272       142.26       145.09       143.80	Fri Apr  8 00:06:18 2022
        32768        38664       255.26       258.12       256.73	Fri Apr  8 00:06:30 2022
       131072        12876       755.98       760.53       758.25	Fri Apr  8 00:06:41 2022
       524288         3425      3168.45      3220.63      3191.38	Fri Apr  8 00:06:52 2022


# All processes entering MPI_Finalize

[3491620 ## mn003: 32173 ## 2022-04-08/00:07:04] job_finished. use-time: 75.893669 (secs)
dispatching ...
Job 3491620 has been finished.
