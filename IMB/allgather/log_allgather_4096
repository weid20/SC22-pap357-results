************************************************ export SWCH_USE_SLAVE_COMMU=0;  export SWCH_USE_META=0 **************************************************
Job <3491573> has been submitted to queue <q_share>
waiting for dispatch ...
[3491573 ## mn388: 16019 ## 2022-04-07/23:36:04] after query job cnodelist from db, use-time 0.20 secs.
[3491573 ## mn388: 16019 ## 2022-04-07/23:36:04] cnodenum= 1024, job-resource-list: 9728-10239,61696-61951,63744-63999[mpemap:0xf][spemap:0x00000000000000000000000000000000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF]
[3491573 ## mn388: 16019 ## 2022-04-07/23:36:04] control node is: mn388.
[3491573 ## mn388: 16019 ## 2022-04-07/23:36:04] sub control node number is 4, list: mn[30,38,46,54].
[3491573 ## mn388: 16019 ## 2022-04-07/23:36:05] after gather all tasks. use-time: 0.72 secs.
[3491573 ## mn388: 16019 ## 2022-04-07/23:36:05] after put tasks info to db, use-time 0.07 secs.
[3491573 ## mn388: 16019 ## 2022-04-07/23:36:05] start job ok, cnodenum=1024, total use-time=1.14 secs!
[3491573 ## vn009728: 20563 ## 2022-04-07/23:36:07] job's first task will run: /usr/bin/swrun -b 1 -T 1024 -s 8000 -c 0 -j 3491573 -F 0xFFFFFFFFFFFFFFFF -i ./IMB-MPI1 -iter 10000,5120,20 -npmin 16384 allgather -msglen len_allgather -sync 1 -mem 4.0 
 benchmarks to run allgather 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.1, MPI-1 part    
#------------------------------------------------------------
# Date                  : Thu Apr  7 23:36:13 2022
# Machine               : sw_64
# System                : Linux
# Release               : 4.4.15-g6fff048
# Version               : #239 SMP Fri May 14 16:06:39 CST 2021
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# ./IMB-MPI1 -iter 10000 -npmin 16384 allgather -msglen len_allgather
#            -sync 1 -mem 4.0

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Allgather

#----------------------------------------------------------------
# Benchmarking Allgather 
# #processes = 4096 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
         2048          800      3075.91     12368.32      5664.46	Thu Apr  7 23:36:24 2022
         8192          208     11415.72     48409.26     21411.53	Thu Apr  7 23:36:36 2022
        32768           52     45528.48    192830.64     85130.86	Thu Apr  7 23:36:47 2022
       131072           14    177486.67    766974.57    335952.81	Thu Apr  7 23:36:59 2022


# All processes entering MPI_Finalize

[3491573 ## mn388: 16019 ## 2022-04-07/23:37:29] job_finished. use-time: 83.885954 (secs)
dispatching ...
Job 3491573 has been finished.
************************************************ export SWCH_USE_SLAVE_COMMU=1;  export SWCH_USE_META=0 **************************************************
Job <3491576> has been submitted to queue <q_share>
waiting for dispatch ...
[3491576 ## mn391: 20705 ## 2022-04-07/23:37:51] after query job cnodelist from db, use-time 0.25 secs.
[3491576 ## mn391: 20705 ## 2022-04-07/23:37:51] cnodenum= 1024, job-resource-list: 9728-10239,61696-61951,63744-63999[mpemap:0xf][spemap:0x00000000000000000000000000000000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF]
[3491576 ## mn391: 20705 ## 2022-04-07/23:37:51] control node is: mn391.
[3491576 ## mn391: 20705 ## 2022-04-07/23:37:51] sub control node number is 4, list: mn[45,53,61,69].
[3491576 ## mn391: 20705 ## 2022-04-07/23:37:52] after gather all tasks. use-time: 0.71 secs.
[3491576 ## mn391: 20705 ## 2022-04-07/23:37:52] after put tasks info to db, use-time 0.08 secs.
[3491576 ## mn391: 20705 ## 2022-04-07/23:37:52] start job ok, cnodenum=1024, total use-time=1.22 secs!
[3491576 ## vn009728: 20610 ## 2022-04-07/23:37:54] job's first task will run: /usr/bin/swrun -b 1 -T 1024 -s 8000 -c 0 -j 3491576 -F 0xFFFFFFFFFFFFFFFF -i ./IMB-MPI1 -iter 10000,5120,20 -npmin 16384 allgather -msglen len_allgather -sync 1 -mem 4.0 
 benchmarks to run allgather 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.1, MPI-1 part    
#------------------------------------------------------------
# Date                  : Thu Apr  7 23:38:01 2022
# Machine               : sw_64
# System                : Linux
# Release               : 4.4.15-g6fff048
# Version               : #239 SMP Fri May 14 16:06:39 CST 2021
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# ./IMB-MPI1 -iter 10000 -npmin 16384 allgather -msglen len_allgather
#            -sync 1 -mem 4.0

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Allgather

#----------------------------------------------------------------
# Benchmarking Allgather 
# #processes = 4096 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
         2048          813      3055.07     12292.28      5593.61	Thu Apr  7 23:38:12 2022
         8192          207     11293.63     48190.98     21237.47	Thu Apr  7 23:38:23 2022
        32768           53     45433.63    192611.36     85056.30	Thu Apr  7 23:38:35 2022
       131072           14    176874.57    767733.49    336831.88	Thu Apr  7 23:38:47 2022


# All processes entering MPI_Finalize

[3491576 ## mn391: 20705 ## 2022-04-07/23:39:16] job_finished. use-time: 83.841968 (secs)
dispatching ...
Job 3491576 has been finished.
************************************************ export SWCH_USE_SLAVE_COMMU=0;  export SWCH_USE_META=1 **************************************************
Job <3491579> has been submitted to queue <q_share>
waiting for dispatch ...
[3491579 ## mn394: 26243 ## 2022-04-07/23:39:41] after query job cnodelist from db, use-time 0.18 secs.
[3491579 ## mn394: 26243 ## 2022-04-07/23:39:41] cnodenum= 1024, job-resource-list: 9728-10239,61696-61951,63744-63999[mpemap:0xf][spemap:0x00000000000000000000000000000000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF]
[3491579 ## mn394: 26243 ## 2022-04-07/23:39:41] control node is: mn394.
[3491579 ## mn394: 26243 ## 2022-04-07/23:39:41] sub control node number is 4, list: mn[100,108,116,124].
[3491579 ## mn394: 26243 ## 2022-04-07/23:39:41] after gather all tasks. use-time: 0.72 secs.
[3491579 ## mn394: 26243 ## 2022-04-07/23:39:42] after put tasks info to db, use-time 0.09 secs.
[3491579 ## mn394: 26243 ## 2022-04-07/23:39:42] start job ok, cnodenum=1024, total use-time=1.17 secs!
[3491579 ## vn009728: 20651 ## 2022-04-07/23:39:44] job's first task will run: /usr/bin/swrun -b 1 -T 1024 -s 8000 -c 0 -j 3491579 -F 0xFFFFFFFFFFFFFFFF -i ./IMB-MPI1 -iter 10000,5120,20 -npmin 16384 allgather -msglen len_allgather -sync 1 -mem 4.0 
 benchmarks to run allgather 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.1, MPI-1 part    
#------------------------------------------------------------
# Date                  : Thu Apr  7 23:39:50 2022
# Machine               : sw_64
# System                : Linux
# Release               : 4.4.15-g6fff048
# Version               : #239 SMP Fri May 14 16:06:39 CST 2021
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# ./IMB-MPI1 -iter 10000 -npmin 16384 allgather -msglen len_allgather
#            -sync 1 -mem 4.0

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Allgather

#----------------------------------------------------------------
# Benchmarking Allgather 
# #processes = 4096 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
         2048         6255      1543.81      1556.99      1556.66	Thu Apr  7 23:40:03 2022
         8192         1835      5254.20      5294.29      5272.50	Thu Apr  7 23:40:17 2022
        32768          490     19295.59     19545.70     19420.03	Thu Apr  7 23:40:29 2022
       131072          124     78731.79     79726.77     79174.91	Thu Apr  7 23:40:42 2022


# All processes entering MPI_Finalize

[3491579 ## mn394: 26243 ## 2022-04-07/23:41:17] job_finished. use-time: 95.833663 (secs)
dispatching ...
Job 3491579 has been finished.
************************************************ export SWCH_USE_SLAVE_COMMU=1;  export SWCH_USE_META=1 **************************************************
Job <3491582> has been submitted to queue <q_share>
waiting for dispatch ...
[3491582 ## mn397: 29655 ## 2022-04-07/23:41:41] after query job cnodelist from db, use-time 0.18 secs.
[3491582 ## mn397: 29655 ## 2022-04-07/23:41:41] cnodenum= 1024, job-resource-list: 9728-10239,61696-61951,63744-63999[mpemap:0xf][spemap:0x00000000000000000000000000000000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF]
[3491582 ## mn397: 29655 ## 2022-04-07/23:41:41] control node is: mn397.
[3491582 ## mn397: 29655 ## 2022-04-07/23:41:41] sub control node number is 4, list: mn[312,320,328,336].
[3491582 ## mn397: 29655 ## 2022-04-07/23:41:42] after gather all tasks. use-time: 0.71 secs.
[3491582 ## mn397: 29655 ## 2022-04-07/23:41:42] after put tasks info to db, use-time 0.08 secs.
[3491582 ## mn397: 29655 ## 2022-04-07/23:41:42] start job ok, cnodenum=1024, total use-time=1.15 secs!
[3491582 ## vn009728: 20710 ## 2022-04-07/23:41:44] job's first task will run: /usr/bin/swrun -b 1 -T 1024 -s 8000 -c 0 -j 3491582 -F 0xFFFFFFFFFFFFFFFF -i ./IMB-MPI1 -iter 10000,5120,20 -npmin 16384 allgather -msglen len_allgather -sync 1 -mem 4.0 
 benchmarks to run allgather 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.1, MPI-1 part    
#------------------------------------------------------------
# Date                  : Thu Apr  7 23:41:50 2022
# Machine               : sw_64
# System                : Linux
# Release               : 4.4.15-g6fff048
# Version               : #239 SMP Fri May 14 16:06:39 CST 2021
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# ./IMB-MPI1 -iter 10000 -npmin 16384 allgather -msglen len_allgather
#            -sync 1 -mem 4.0

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Allgather

#----------------------------------------------------------------
# Benchmarking Allgather 
# #processes = 4096 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
         2048         9268		 948.07       983.77       958.50	Thu Apr  7 23:42:05 2022
         8192         3098      3274.57      3462.66      3302.56	Thu Apr  7 23:42:20 2022
        32768          759     13180.27     14278.05     13312.12	Thu Apr  7 23:42:33 2022
       131072          185     52677.13     57574.71     53149.77	Thu Apr  7 23:42:46 2022


# All processes entering MPI_Finalize

[3491582 ## mn397: 29655 ## 2022-04-07/23:43:19] job_finished. use-time: 97.405599 (secs)
dispatching ...
Job 3491582 has been finished.
