************************* export SWCH_USE_SLAVE_CALCU=0;  export SWCH_USE_SLAVE_COMMU=0;  export SWCH_USE_SLAVE_FUSION=0 *******************************************
Job <3326234> has been submitted to queue <q_linpack_test>
waiting for dispatch ...
[3326234 ## mn154: 26628 ## 2021-12-10/20:24:20] after query job cnodelist from db, use-time 0.22 secs.
[3326234 ## mn154: 26628 ## 2021-12-10/20:24:20] cnodenum= 64, job-resource-list: 86016-86079[mpemap:0xf][spemap:0x00000000000000000000000000000000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF]
[3326234 ## mn154: 26628 ## 2021-12-10/20:24:20] control node is: mn154.
[3326234 ## mn154: 26628 ## 2021-12-10/20:24:20] sub control node number is 1, list: mn173.
[3326234 ## mn154: 26628 ## 2021-12-10/20:24:20] after gather all tasks. use-time: 0.18 secs.
[3326234 ## mn154: 26628 ## 2021-12-10/20:24:20] after put tasks info to db, use-time 0.05 secs.
[3326234 ## mn154: 26628 ## 2021-12-10/20:24:20] start job ok, cnodenum=64, total use-time=0.61 secs!
[3326234 ## vn086016: 766 ## 2021-12-10/20:24:22] job's first task will run: /usr/bin/swrun -b 1 -T 1024 -s 8000 -c 0 -j 3326234 -F 0xFFFFFFFFFFFFFFFF -i ./IMB-MPI1 -iter 10000,5120,20 -sync 1 -npmin 4096 allreduce -root_shift 0 -msglen len -mem 4.0 
 benchmarks to run allreduce 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.1, MPI-1 part    
#------------------------------------------------------------
# Date                  : Fri Dec 10 20:24:24 2021
# Machine               : sw_64
# System                : Linux
# Release               : 4.4.15-ga9d5080-dirty
# Version               : #153 SMP Sat Dec 5 14:51:32 CST 2020
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# ./IMB-MPI1 -iter 10000 -sync 1 -npmin 4096
#            allreduce -root_shift 0 -msglen len
#            -mem 4.0

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Allreduce

#----------------------------------------------------------------
# Benchmarking Allreduce 
# #processes = 256 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
        16384        43492       206.36       219.67       208.68	Fri Dec 10 20:25:55 2021
        65536        32092       308.55       311.31       309.91	Fri Dec 10 20:26:07 2021
       262144        14599       697.11       700.70       698.68	Fri Dec 10 20:26:19 2021
      1048576         5883      1710.79      1715.40      1713.20	Fri Dec 10 20:26:31 2021
      4194304         1732      5796.72      5804.89      5800.96	Fri Dec 10 20:26:42 2021
     16777216          452     22071.46     22091.48     22081.46	Fri Dec 10 20:26:53 2021
     67108864          116     85931.69     86011.15     85964.55	Fri Dec 10 20:27:05 2021


# All processes entering MPI_Finalize

[3326234 ## mn154: 26628 ## 2021-12-10/20:27:17] job_finished. use-time: 176.922957 (secs)
dispatching ...
Job 3326234 has been finished.
************************* export SWCH_USE_SLAVE_CALCU=1;  export SWCH_USE_SLAVE_COMMU=0;  export SWCH_USE_SLAVE_FUSION=0 *******************************************
Job <3326235> has been submitted to queue <q_linpack_test>
waiting for dispatch ...
[3326235 ## mn155: 25345 ## 2021-12-10/20:27:40] after query job cnodelist from db, use-time 0.22 secs.
[3326235 ## mn155: 25345 ## 2021-12-10/20:27:40] cnodenum= 64, job-resource-list: 86016-86079[mpemap:0xf][spemap:0x00000000000000000000000000000000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF]
[3326235 ## mn155: 25345 ## 2021-12-10/20:27:40] control node is: mn155.
[3326235 ## mn155: 25345 ## 2021-12-10/20:27:40] sub control node number is 1, list: mn342.
[3326235 ## mn155: 25345 ## 2021-12-10/20:27:40] after gather all tasks. use-time: 0.19 secs.
[3326235 ## mn155: 25345 ## 2021-12-10/20:27:40] after put tasks info to db, use-time 0.07 secs.
[3326235 ## mn155: 25345 ## 2021-12-10/20:27:40] start job ok, cnodenum=64, total use-time=0.67 secs!
[3326235 ## vn086016: 783 ## 2021-12-10/20:27:42] job's first task will run: /usr/bin/swrun -b 1 -T 1024 -s 8000 -c 0 -j 3326235 -F 0xFFFFFFFFFFFFFFFF -i ./IMB-MPI1 -iter 10000,5120,20 -sync 1 -npmin 4096 allreduce -root_shift 0 -msglen len -mem 4.0 
 benchmarks to run allreduce 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.1, MPI-1 part    
#------------------------------------------------------------
# Date                  : Fri Dec 10 20:27:45 2021
# Machine               : sw_64
# System                : Linux
# Release               : 4.4.15-ga9d5080-dirty
# Version               : #153 SMP Sat Dec 5 14:51:32 CST 2020
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# ./IMB-MPI1 -iter 10000 -sync 1 -npmin 4096
#            allreduce -root_shift 0 -msglen len
#            -mem 4.0

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Allreduce

#----------------------------------------------------------------
# Benchmarking Allreduce 
# #processes = 256 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
        16384        43135       207.28       222.63       209.89	Fri Dec 10 20:29:16 2021
        65536        34100       276.17       290.72       278.29	Fri Dec 10 20:29:28 2021
       262144        24580       392.48       407.88       395.09	Fri Dec 10 20:29:40 2021
      1048576        13868       722.90       725.23       724.06	Fri Dec 10 20:29:52 2021
      4194304         4993      1988.19      1992.95      1990.23	Fri Dec 10 20:30:03 2021
     16777216         1416      7079.39      7087.46      7083.43	Fri Dec 10 20:30:14 2021
     67108864          358     27948.91     28004.96     27975.98	Fri Dec 10 20:30:25 2021


# All processes entering MPI_Finalize

[3326235 ## mn155: 25345 ## 2021-12-10/20:30:47] job_finished. use-time: 186.419862 (secs)
dispatching ...
Job 3326235 has been finished.
************************* export SWCH_USE_SLAVE_CALCU=1;  export SWCH_USE_SLAVE_COMMU=1;  export SWCH_USE_SLAVE_FUSION=0 *******************************************
Job <3326236> has been submitted to queue <q_linpack_test>
waiting for dispatch ...
[3326236 ## mn156: 29759 ## 2021-12-10/20:31:10] after query job cnodelist from db, use-time 0.22 secs.
[3326236 ## mn156: 29759 ## 2021-12-10/20:31:10] cnodenum= 64, job-resource-list: 86016-86079[mpemap:0xf][spemap:0x00000000000000000000000000000000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF]
[3326236 ## mn156: 29759 ## 2021-12-10/20:31:10] control node is: mn156.
[3326236 ## mn156: 29759 ## 2021-12-10/20:31:10] sub control node number is 1, list: mn198.
[3326236 ## mn156: 29759 ## 2021-12-10/20:31:11] after gather all tasks. use-time: 0.18 secs.
[3326236 ## mn156: 29759 ## 2021-12-10/20:31:11] after put tasks info to db, use-time 0.06 secs.
[3326236 ## mn156: 29759 ## 2021-12-10/20:31:11] start job ok, cnodenum=64, total use-time=0.60 secs!
[3326236 ## vn086016: 800 ## 2021-12-10/20:31:13] job's first task will run: /usr/bin/swrun -b 1 -T 1024 -s 8000 -c 0 -j 3326236 -F 0xFFFFFFFFFFFFFFFF -i ./IMB-MPI1 -iter 10000,5120,20 -sync 1 -npmin 4096 allreduce -root_shift 0 -msglen len -mem 4.0 
 benchmarks to run allreduce 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.1, MPI-1 part    
#------------------------------------------------------------
# Date                  : Fri Dec 10 20:31:15 2021
# Machine               : sw_64
# System                : Linux
# Release               : 4.4.15-ga9d5080-dirty
# Version               : #153 SMP Sat Dec 5 14:51:32 CST 2020
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# ./IMB-MPI1 -iter 10000 -sync 1 -npmin 4096
#			 allreduce -root_shift 0 -msglen len
#            -mem 4.0

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Allreduce

#----------------------------------------------------------------
# Benchmarking Allreduce 
# #processes = 256 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
        16384        48366       207.80       210.38       208.87	Fri Dec 10 20:32:47 2021
        65536        35231       278.03       280.34       278.90	Fri Dec 10 20:33:00 2021
       262144        25613       389.23       392.27       390.91	Fri Dec 10 20:33:12 2021
      1048576        16159       615.66       619.37       617.66	Fri Dec 10 20:33:24 2021
      4194304         6867      1436.08      1440.25      1438.11	Fri Dec 10 20:33:35 2021
     16777216         1985      5011.75      5027.43      5019.12	Fri Dec 10 20:33:46 2021
     67108864          475     21014.23     21096.09     21061.12	Fri Dec 10 20:33:58 2021


# All processes entering MPI_Finalize

[3326236 ## mn156: 29759 ## 2021-12-10/20:34:19] job_finished. use-time: 188.418275 (secs)
dispatching ...
Job 3326236 has been finished.
************************* export SWCH_USE_SLAVE_CALCU=1;  export SWCH_USE_SLAVE_COMMU=1;  export SWCH_USE_SLAVE_FUSION=1 *******************************************
Job <3326237> has been submitted to queue <q_linpack_test>
waiting for dispatch ...
[3326237 ## mn157: 18014 ## 2021-12-10/20:34:42] after query job cnodelist from db, use-time 0.17 secs.
[3326237 ## mn157: 18014 ## 2021-12-10/20:34:42] cnodenum= 64, job-resource-list: 86016-86079[mpemap:0xf][spemap:0x00000000000000000000000000000000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF]
[3326237 ## mn157: 18014 ## 2021-12-10/20:34:42] control node is: mn157.
[3326237 ## mn157: 18014 ## 2021-12-10/20:34:42] sub control node number is 1, list: mn073.
[3326237 ## mn157: 18014 ## 2021-12-10/20:34:42] after gather all tasks. use-time: 0.19 secs.
[3326237 ## mn157: 18014 ## 2021-12-10/20:34:42] after put tasks info to db, use-time 0.07 secs.
[3326237 ## mn157: 18014 ## 2021-12-10/20:34:42] start job ok, cnodenum=64, total use-time=0.58 secs!
[3326237 ## vn086016: 816 ## 2021-12-10/20:34:44] job's first task will run: /usr/bin/swrun -b 1 -T 1024 -s 8000 -c 0 -j 3326237 -F 0xFFFFFFFFFFFFFFFF -i ./IMB-MPI1 -iter 10000,5120,20 -sync 1 -npmin 4096 allreduce -root_shift 0 -msglen len -mem 4.0 
 benchmarks to run allreduce 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.1, MPI-1 part    
#------------------------------------------------------------
# Date                  : Fri Dec 10 20:34:46 2021
# Machine               : sw_64
# System                : Linux
# Release               : 4.4.15-ga9d5080-dirty
# Version               : #153 SMP Sat Dec 5 14:51:32 CST 2020
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# ./IMB-MPI1 -iter 10000 -sync 1 -npmin 4096
#            allreduce -root_shift 0 -msglen len
#            -mem 4.0

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Allreduce

#----------------------------------------------------------------
# Benchmarking Allreduce 
# #processes = 256 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
        16384        47725       210.08       212.38       211.28	Fri Dec 10 20:36:19 2021
        65536        37679       253.48       256.14       254.47	Fri Dec 10 20:36:32 2021
       262144        28497       349.45       353.15       351.40	Fri Dec 10 20:36:44 2021
      1048576        17961       548.54       552.56       550.57	Fri Dec 10 20:36:56 2021
      4194304         7888      1259.69      1262.86      1261.55	Fri Dec 10 20:37:08 2021
     16777216         2252      4409.68      4428.65      4418.29	Fri Dec 10 20:37:19 2021
     67108864          536     18511.99     18584.62     18553.08	Fri Dec 10 20:37:30 2021


# All processes entering MPI_Finalize

[3326237 ## mn157: 18014 ## 2021-12-10/20:37:49] job_finished. use-time: 186.461620 (secs)
dispatching ...
Job 3326237 has been finished.
