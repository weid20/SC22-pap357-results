************************* export SWCH_USE_SLAVE_CALCU=0;  export SWCH_USE_SLAVE_COMMU=0;  export SWCH_USE_SLAVE_FUSION=0 *******************************************
Job <3326244> has been submitted to queue <q_linpack_test>
waiting for dispatch ...
[3326244 ## mn165: 3891 ## 2021-12-10/20:53:36] after query job cnodelist from db, use-time 0.17 secs.
[3326244 ## mn165: 3891 ## 2021-12-10/20:53:36] cnodenum= 1024, job-resource-list: 86016-87039[mpemap:0xf][spemap:0x00000000000000000000000000000000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF]
[3326244 ## mn165: 3891 ## 2021-12-10/20:53:36] control node is: mn165.
[3326244 ## mn165: 3891 ## 2021-12-10/20:53:36] sub control node number is 4, list: mn[269,277,285,293].
[3326244 ## mn165: 3891 ## 2021-12-10/20:53:36] after gather all tasks. use-time: 0.36 secs.
[3326244 ## mn165: 3891 ## 2021-12-10/20:53:36] after put tasks info to db, use-time 0.08 secs.
[3326244 ## mn165: 3891 ## 2021-12-10/20:53:37] start job ok, cnodenum=1024, total use-time=0.85 secs!
[3326244 ## vn086016: 897 ## 2021-12-10/20:53:39] job's first task will run: /usr/bin/swrun -b 1 -T 1024 -s 8000 -c 0 -j 3326244 -F 0xFFFFFFFFFFFFFFFF -i ./IMB-MPI1 -iter 10000,5120,20 -sync 1 -npmin 4096 allreduce -root_shift 0 -msglen len -mem 4.0 
 benchmarks to run allreduce 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.1, MPI-1 part    
#------------------------------------------------------------
# Date                  : Fri Dec 10 20:53:45 2021
# Machine               : sw_64
# System                : Linux
# Release               : 4.4.15-ga9d5080-dirty
# Version               : #153 SMP Sat Dec 5 14:51:32 CST 2020
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# ./IMB-MPI1 -iter 10000 -sync 1 -npmin 4096
#            allreduce -root_shift 0 -msglen len
#            -mem 4.0

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Allreduce

#----------------------------------------------------------------
# Benchmarking Allreduce 
# #processes = 4096 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
        16384        16315       557.50       569.28       564.84	Fri Dec 10 20:55:21 2021
        65536        12288       699.95       712.54       707.73	Fri Dec 10 20:55:33 2021
       262144         7477      1264.77      1280.47      1274.31	Fri Dec 10 20:55:46 2021
      1048576         3905      2480.42      2497.75      2489.76	Fri Dec 10 20:55:59 2021
      4194304         1402      6900.51      6923.50      6912.69	Fri Dec 10 20:56:10 2021
     16777216          432     23250.09     23293.56     23272.15	Fri Dec 10 20:56:21 2021
     67108864          115     87244.51     87400.37     87307.82	Fri Dec 10 20:56:33 2021


# All processes entering MPI_Finalize

[3326244 ## mn165: 3891 ## 2021-12-10/20:56:54] job_finished. use-time: 197.461786 (secs)
dispatching ...
Job 3326244 has been finished.
************************* export SWCH_USE_SLAVE_CALCU=1;  export SWCH_USE_SLAVE_COMMU=0;  export SWCH_USE_SLAVE_FUSION=0 *******************************************
Job <3326245> has been submitted to queue <q_linpack_test>
waiting for dispatch ...
[3326245 ## mn166: 15783 ## 2021-12-10/20:57:18] after query job cnodelist from db, use-time 0.17 secs.
[3326245 ## mn166: 15783 ## 2021-12-10/20:57:18] cnodenum= 1024, job-resource-list: 86016-87039[mpemap:0xf][spemap:0x00000000000000000000000000000000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF]
[3326245 ## mn166: 15783 ## 2021-12-10/20:57:18] control node is: mn166.
[3326245 ## mn166: 15783 ## 2021-12-10/20:57:18] sub control node number is 4, list: mn[432,1,9,17].
[3326245 ## mn166: 15783 ## 2021-12-10/20:57:18] after gather all tasks. use-time: 0.36 secs.
[3326245 ## mn166: 15783 ## 2021-12-10/20:57:18] after put tasks info to db, use-time 0.06 secs.
[3326245 ## mn166: 15783 ## 2021-12-10/20:57:18] start job ok, cnodenum=1024, total use-time=0.83 secs!
[3326245 ## vn086016: 913 ## 2021-12-10/20:57:20] job's first task will run: /usr/bin/swrun -b 1 -T 1024 -s 8000 -c 0 -j 3326245 -F 0xFFFFFFFFFFFFFFFF -i ./IMB-MPI1 -iter 10000,5120,20 -sync 1 -npmin 4096 allreduce -root_shift 0 -msglen len -mem 4.0 
 benchmarks to run allreduce 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.1, MPI-1 part    
#------------------------------------------------------------
# Date                  : Fri Dec 10 20:57:27 2021
# Machine               : sw_64
# System                : Linux
# Release               : 4.4.15-ga9d5080-dirty
# Version               : #153 SMP Sat Dec 5 14:51:32 CST 2020
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# ./IMB-MPI1 -iter 10000 -sync 1 -npmin 4096
#            allreduce -root_shift 0 -msglen len
#            -mem 4.0

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Allreduce

#----------------------------------------------------------------
# Benchmarking Allreduce 
# #processes = 4096 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
        16384        18104       510.37       521.91       517.26	Fri Dec 10 20:59:01 2021
        65536        18104       601.39       613.21       608.02	Fri Dec 10 20:59:17 2021
       262144        12823       745.66       758.35       753.34	Fri Dec 10 20:59:30 2021
      1048576         7341      1139.44      1154.70      1148.12	Fri Dec 10 20:59:41 2021
      4194304         4001      2565.76      2581.81      2574.85	Fri Dec 10 20:59:54 2021
     16777216         1290      7692.68      7719.67      7708.90	Fri Dec 10 21:00:05 2021
     67108864          350     28363.05     28458.71     28410.52	Fri Dec 10 21:00:17 2021


# All processes entering MPI_Finalize

[3326245 ## mn166: 15783 ## 2021-12-10/21:00:35] job_finished. use-time: 196.414528 (secs)
dispatching ...
Job 3326245 has been finished.
************************* export SWCH_USE_SLAVE_CALCU=1;  export SWCH_USE_SLAVE_COMMU=1;  export SWCH_USE_SLAVE_FUSION=0 *******************************************
Job <3326246> has been submitted to queue <q_linpack_test>
waiting for dispatch ...
[3326246 ## mn167: 14132 ## 2021-12-10/21:00:57] after query job cnodelist from db, use-time 0.19 secs.
[3326246 ## mn167: 14132 ## 2021-12-10/21:00:57] cnodenum= 1024, job-resource-list: 86016-87039[mpemap:0xf][spemap:0x00000000000000000000000000000000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF]
[3326246 ## mn167: 14132 ## 2021-12-10/21:00:57] control node is: mn167.
[3326246 ## mn167: 14132 ## 2021-12-10/21:00:57] sub control node number is 4, list: mn[190,198,206,214].
[3326246 ## mn167: 14132 ## 2021-12-10/21:00:58] after gather all tasks. use-time: 0.36 secs.
[3326246 ## mn167: 14132 ## 2021-12-10/21:01:00] after put tasks info to db, use-time 2.14 secs.
[3326246 ## mn167: 14132 ## 2021-12-10/21:01:00] start job ok, cnodenum=1024, total use-time=2.94 secs!
[3326246 ## vn086016: 929 ## 2021-12-10/21:01:02] job's first task will run: /usr/bin/swrun -b 1 -T 1024 -s 8000 -c 0 -j 3326246 -F 0xFFFFFFFFFFFFFFFF -i ./IMB-MPI1 -iter 10000,5120,20 -sync 1 -npmin 4096 allreduce -root_shift 0 -msglen len -mem 4.0 
 benchmarks to run allreduce 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.1, MPI-1 part    
#------------------------------------------------------------
# Date                  : Fri Dec 10 21:01:09 2021
# Machine               : sw_64
# System                : Linux
# Release               : 4.4.15-ga9d5080-dirty
# Version               : #153 SMP Sat Dec 5 14:51:32 CST 2020
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# ./IMB-MPI1 -iter 10000 -sync 1 -npmin 4096
#            allreduce -root_shift 0 -msglen len
#            -mem 4.0

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Allreduce

#----------------------------------------------------------------
# Benchmarking Allreduce 
# #processes = 4096 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
        16384        16655       507.86       518.32       513.92	Fri Dec 10 21:02:44 2021
        65536        16655       584.06       595.08       590.40	Fri Dec 10 21:02:58 2021
       262144        11282       730.46       744.17       738.57	Fri Dec 10 21:03:10 2021
      1048576         9479      1039.70      1055.44      1048.94	Fri Dec 10 21:03:23 2021
      4194304         4836      2036.01      2054.99      2046.70	Fri Dec 10 21:03:36 2021
     16777216         1780      5816.06      5854.35      5838.35	Fri Dec 10 21:03:48 2021
     67108864          457     21826.84     21922.56     21879.71	Fri Dec 10 21:04:00 2021


# All processes entering MPI_Finalize

[3326246 ## mn167: 14132 ## 2021-12-10/21:04:15] job_finished. use-time: 194.924714 (secs)
dispatching ...
Job 3326246 has been finished.
************************* export SWCH_USE_SLAVE_CALCU=1;  export SWCH_USE_SLAVE_COMMU=1;  export SWCH_USE_SLAVE_FUSION=1 *******************************************
Job <3326247> has been submitted to queue <q_linpack_test>
waiting for dispatch ...
[3326247 ## mn168: 23203 ## 2021-12-10/21:04:38] after query job cnodelist from db, use-time 0.16 secs.
[3326247 ## mn168: 23203 ## 2021-12-10/21:04:38] cnodenum= 1024, job-resource-list: 86016-87039[mpemap:0xf][spemap:0x00000000000000000000000000000000FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF]
[3326247 ## mn168: 23203 ## 2021-12-10/21:04:38] control node is: mn168.
[3326247 ## mn168: 23203 ## 2021-12-10/21:04:38] sub control node number is 4, list: mn[346,354,362,370].
[3326247 ## mn168: 23203 ## 2021-12-10/21:04:38] after gather all tasks. use-time: 0.37 secs.
[3326247 ## mn168: 23203 ## 2021-12-10/21:04:39] after put tasks info to db, use-time 0.08 secs.
[3326247 ## mn168: 23203 ## 2021-12-10/21:04:39] start job ok, cnodenum=1024, total use-time=0.89 secs!
[3326247 ## vn086016: 945 ## 2021-12-10/21:04:41] job's first task will run: /usr/bin/swrun -b 1 -T 1024 -s 8000 -c 0 -j 3326247 -F 0xFFFFFFFFFFFFFFFF -i ./IMB-MPI1 -iter 10000,5120,20 -sync 1 -npmin 4096 allreduce -root_shift 0 -msglen len -mem 4.0 
 benchmarks to run allreduce 
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.1, MPI-1 part    
#------------------------------------------------------------
# Date                  : Fri Dec 10 21:04:47 2021
# Machine               : sw_64
# System                : Linux
# Release               : 4.4.15-ga9d5080-dirty
# Version               : #153 SMP Sat Dec 5 14:51:32 CST 2020
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# ./IMB-MPI1 -iter 10000 -sync 1 -npmin 4096
#            allreduce -root_shift 0 -msglen len
#            -mem 4.0

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Allreduce

#----------------------------------------------------------------
# Benchmarking Allreduce 
# #processes = 4096 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
        16384        19729       502.31       512.32       508.17	Fri Dec 10 21:06:18 2021
        65536        16817       554.46       565.96       560.98	Fri Dec 10 21:06:31 2021
       262144        16366       677.96       691.80       686.10	Fri Dec 10 21:06:47 2021
      1048576         9894       953.28       967.90       962.28	Fri Dec 10 21:07:00 2021
      4194304         5216      1804.05      1820.45      1813.73	Fri Dec 10 21:07:12 2021
     16777216         1865      5112.19      5146.81      5130.56	Fri Dec 10 21:07:24 2021
     67108864          517     19277.93     19387.55     19337.30	Fri Dec 10 21:07:35 2021


# All processes entering MPI_Finalize

[3326247 ## mn168: 23203 ## 2021-12-10/21:07:57] job_finished. use-time: 197.984308 (secs)
dispatching ...
Job 3326247 has been finished.
